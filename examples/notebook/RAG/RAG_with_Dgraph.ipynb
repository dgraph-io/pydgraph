{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac5a3238",
   "metadata": {},
   "source": [
    "# Product recommendation using RAG and Dgraph Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b1090",
   "metadata": {},
   "source": [
    "In this notebook we will use Dgraph database to store retail products information (Amazon products data) and Language Models to reply to users asking for a **product recommendation**.\n",
    "\n",
    "The language models are used in 3 different ways\n",
    "- we use LLM text analysis capabilities to craft a Graph database query to fetch the relevant information to generate a response. \n",
    "- we use a small model to generate and store text embeddings to find products, categories, brand, characteristics etc. based on semantic similarity\n",
    "- we use a LLM to generate a response to the use question based on the data retrieved in the Graph.\n",
    "\n",
    "This is a case of Retrieval Augmented Generation (RAG), and NLP (Natural Language Processing) leveraging graph structures.\n",
    "\n",
    "### Why Dgraph?\n",
    "\n",
    "Dgraph is particularly suited for knowledge graph and AI applications due to several key features and capabilities:\n",
    "\n",
    "- Graph Database Structure: Dgraph is designed as a native graph database, which means it stores data in a graph structure consisting of nodes, edges, and properties. This is inherently aligned with the way knowledge graphs represent relationships and entities, making it easier to model complex interconnections.\n",
    "\n",
    "- Native vector support: Any node may have any number of vector predicates which are indexed using HNSW algorithm for fast similarity retrieval.\n",
    "\n",
    "- Scalability: Dgraph is built to scale horizontally, handling large volumes of data and high query loads efficiently. This is crucial for AI applications that often require processing vast amounts of interconnected data.\n",
    "\n",
    "- High Performance: Dgraph provides fast query execution and low latency, which are essential for real-time AI applications. Its performance optimizations, such as parallel query execution and efficient data storage, make it capable of handling demanding workloads.\n",
    "\n",
    "- Flexible Schema: Dgraph supports flexible schema definitions, allowing for dynamic data models that can evolve over time. This is beneficial for AI applications where the data schema might need to adapt to new requirements or insights.\n",
    "\n",
    "- Rich Querying Capabilities: Dgraphâ€™s query language, DQL (Dgraph Query Language) is declarative, which means that queries return a response back in a similar shape to the query. DQL allows for complex graph traversals and pattern matching, which are essential for extracting insights and relationships in knowledge graphs. It also supports advanced features like recursive queries and aggregations and most importantly vector similarity search.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d9f40c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We just need some python packages for Dgraph, Openai, Hugging Face and some tools we are using.\n",
    "\n",
    "Create a file `.env` in the folder containing this python notebook with one line for your OpenAI API key\n",
    "```\n",
    "OPENAI_API_KEY=sk-....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04d0838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional script to install all the required packages\n",
    "!pip3 install pydgraph\n",
    "!pip3 install openai\n",
    "!pip3 install sentence_transformers\n",
    "!pip3 install pybars3\n",
    "!pip3 install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ff46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pydgraph\n",
    "from pybars import Compiler\n",
    "\n",
    "# Activate the provider you want to use for embeddings and LLM\n",
    "# from openai import OpenAI\n",
    "# from mistralai.client import MistralClient\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\") is not None, \"Set OPENAI_API_KEY in your .env file\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137d9d3",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Dgraph supports JSON and RDF format.\n",
    "In this Notebook we are using RDF. \n",
    "RDF is a powerful notation for knowledge graph. It describes information in triples of the form Subject - Predicate - Object (S-P-O).\n",
    "\n",
    "\n",
    "The original dataset is in JSON format and is 2.7Mb. We have generated an RDF file with the exact same information. The RDF file is only 361 Kb! \n",
    "\n",
    "If you are interested, see [generateRDF](./generateRDF.ipynb) notebook.\n",
    "\n",
    "## Loading dataset\n",
    "### Connecting to Dgraph\n",
    "\n",
    "See [Learning Environment](https://dgraph.io/docs/deploy/installation/single-host-setup/) to setup a docker image with `dgraph:standalone/latest`, or use your on-prem or cloud instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eef9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB credentials\n",
    "if \"DGRAPH_GRPC\" in os.environ:\n",
    "    dgraph_grpc = os.environ[\"DGRAPH_GRPC\"]\n",
    "else:\n",
    "    dgraph_grpc = \"localhost:9080\"\n",
    "\n",
    "# DGRAPH_ADMIN_KEY must be defined in env variables\n",
    "if \"cloud.dgraph\" in dgraph_grpc:\n",
    "    assert \"DGRAPH_ADMIN_KEY\" in os.environ, \"DGRAPH_ADMIN_KEY must be defined\"\n",
    "    APIAdminKey = os.environ[\"DGRAPH_ADMIN_KEY\"]\n",
    "else:\n",
    "    APIAdminKey = None\n",
    "\n",
    "# TRANSFORMER_API_KEY must be defined in env variables\n",
    "# client stub for on-prem requires grpc host:port without protocol\n",
    "# client stub for cloud requires the grpc endpoint of graphql endpoint or base url of the cluster\n",
    "\n",
    "if APIAdminKey is None:\n",
    "    client_stub = pydgraph.DgraphClientStub(dgraph_grpc)\n",
    "else:\n",
    "    client_stub = pydgraph.DgraphClientStub.from_cloud(dgraph_grpc, APIAdminKey)\n",
    "client = pydgraph.DgraphClient(client_stub)\n",
    "print(f\"Connected to DGraph at {dgraph_grpc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68df989b",
   "metadata": {},
   "source": [
    "### House keeping\n",
    "\n",
    "First we clean the DB. You may want to skip this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4fd841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all data including schema from the Dgraph instance. This is a useful\n",
    "# for small examples such as this since it puts Dgraph into a clean state.\n",
    "confirm = input(\"drop schema and all data (y/n)?\")\n",
    "if confirm == \"y\":\n",
    "    op = pydgraph.Operation(drop_all=True)\n",
    "    client.alter(op)\n",
    "    print(\"schema and data deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7af9dc5",
   "metadata": {},
   "source": [
    "### Deploying the Graph schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f46b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add predicates to Dgraph type schema\n",
    "with open(\"data/products.schema\", \"r\") as file:\n",
    "    dqlschema = file.read()\n",
    "    op = pydgraph.Operation(schema=dqlschema)\n",
    "    client.alter(op)\n",
    "    print(\"schema updated:\")\n",
    "    print(dqlschema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0a71c",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8aa0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_rdf(nquads, client):\n",
    "    ret = {}\n",
    "    body = \"\\n\".join(nquads)\n",
    "    if len(nquads) > 0:\n",
    "        txn = client.txn()\n",
    "        try:\n",
    "            res = txn.mutate(set_nquads=body)\n",
    "            txn.commit()\n",
    "            ret[\"nquads\"] = (len(nquads),)\n",
    "            ret[\"total_ns\"] = res.latency.total_ns\n",
    "        except pydgraph.errors.AbortedError as err:\n",
    "            print(\"AbortedError %s\" % i)\n",
    "        except Exception as inst:\n",
    "            print(inst)\n",
    "        finally:\n",
    "            txn.discard()\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "with open(\"data/products.rdf\") as f:\n",
    "    data = f.readlines()\n",
    "    mutate_rdf(data, client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d999acbc",
   "metadata": {},
   "source": [
    "### Simple graph query\n",
    "As our data is now in a graph database, we can traverse the graph, search for nodes, count relationships etc. \n",
    "To verify that we have data in the DB, let's execute a simple query to find the top 5 categories and their number of products, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "82d04d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 categories with the most products:\n",
      "{\n",
      "    \"productsPerCategory\": [\n",
      "        {\n",
      "            \"category\": \"home decoration\",\n",
      "            \"number_of_products\": 20\n",
      "        },\n",
      "        {\n",
      "            \"category\": \"books\",\n",
      "            \"number_of_products\": 17\n",
      "        },\n",
      "        {\n",
      "            \"category\": \"women clothing\",\n",
      "            \"number_of_products\": 17\n",
      "        },\n",
      "        {\n",
      "            \"category\": \"book\",\n",
      "            \"number_of_products\": 14\n",
      "        },\n",
      "        {\n",
      "            \"category\": \"footwear\",\n",
      "            \"number_of_products\": 12\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "{ \n",
    "    var(func:type(category)) { \n",
    "        np as count(~Product.category)\n",
    "    }\n",
    "    productsPerCategory(func:uid(np), orderdesc:val(np), first:5){\n",
    "        category:category.Value\n",
    "        number_of_products:val(np)\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "res = client.txn(read_only=True).query(query)\n",
    "res = json.loads(res.json)\n",
    "print(\"Top 5 categories with the most products:\")\n",
    "print(json.dumps(res, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eaf5b21",
   "metadata": {},
   "source": [
    "## Similarity search with vector embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bac67e7",
   "metadata": {},
   "source": [
    "We don't want to constrain the question to only use terms present in the database. For example, the user may want \"some clothes of dark color\". We need to search our graph by similarity and not only by terms. \n",
    "We will use the power of Dgraph vectors and language model vector embeddings.\n",
    "\n",
    "### Creating vector indexes\n",
    "Dgraph is a Graph database with native vector support, HNSW index, and similarity search. For this use case, we will be using a Python script shared in the blog post [Add OpenAI, Mistral or open-source embeddings to your knowledge graph.](https://dgraph.io/blog/post/embeddings/) to compute and add vector embeddings to all our entities. \n",
    "\n",
    "For example, with an embedding on the `color` entities, we will be able to search for colors `similar_to` \"dark color\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567e35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic functions to manage embeddings\n",
    "compiler = Compiler()\n",
    "\n",
    "\n",
    "def clearIndex(predicate):\n",
    "    print(f\"remove index for {predicate}\")\n",
    "    schema = f\"{predicate}: float32vector .\"\n",
    "    op = pydgraph.Operation(schema=schema)\n",
    "    alter = client.alter(op)\n",
    "    print(alter)\n",
    "\n",
    "\n",
    "def computeIndex(predicate, index):\n",
    "    print(f\"create index for {predicate} {index}\")\n",
    "    schema = f\"{predicate}: float32vector @index({index}) .\"\n",
    "    op = pydgraph.Operation(schema=schema)\n",
    "    alter = client.alter(op)\n",
    "    print(alter)\n",
    "\n",
    "\n",
    "def huggingfaceEmbeddings(model, sentences):\n",
    "    embeddings = model.encode(sentences)\n",
    "    return embeddings.tolist()\n",
    "\n",
    "\n",
    "def computeEmbedding(\n",
    "    predicate, data, template, provider, modelName, model, llm, dimensions\n",
    "):\n",
    "    # data is an array of objects containing uid and other predicates\n",
    "    # create an array of text\n",
    "    # get the embeddings\n",
    "    # produce a RDF text\n",
    "    # data is a list of object having uid and other predicates used in the template\n",
    "\n",
    "    nquad_list = []\n",
    "    sentences = [template(e) for e in data]\n",
    "\n",
    "    if \"huggingface\" == provider:\n",
    "        embeddings = huggingfaceEmbeddings(model, sentences)\n",
    "    elif \"openai\" == provider:\n",
    "        if dimensions != None:\n",
    "            openaidata = llm.embeddings.create(\n",
    "                input=sentences,\n",
    "                model=modelName,\n",
    "                encoding_format=\"float\",\n",
    "                dimensions=dimensions,\n",
    "            )\n",
    "        else:\n",
    "            openaidata = llm.embeddings.create(\n",
    "                input=sentences, model=modelName, encoding_format=\"float\"\n",
    "            )\n",
    "        embeddings = [e.embedding for e in openaidata.data]\n",
    "    elif \"mistral\" == provider:\n",
    "        mistraldata = llm.embeddings(model=modelName, input=sentences)\n",
    "        embeddings = [e.embedding for e in mistraldata.data]\n",
    "\n",
    "    # embeddings is a list of vectors in the same order as the input data\n",
    "    try:\n",
    "        for i in range(0, len(data)):\n",
    "            uid = data[i][\"uid\"]\n",
    "            nquad_list.append(f'<{uid}> <{predicate}> \"{embeddings[i]}\" .')\n",
    "    # (prompt=\"{body[uid]}\")\n",
    "    except Exception as inst:\n",
    "        print(embeddings)\n",
    "    return nquad_list\n",
    "\n",
    "\n",
    "def buildEmbeddings(embedding_def, only_missing=True):\n",
    "    global client\n",
    "    predicate = f\"{embedding_def['entityType']}.{embedding_def['attribute']}\"\n",
    "    if \"disabled\" in embedding_def and embedding_def[\"disabled\"] == True:\n",
    "        print(f\"Predicate {predicate} is disabled\")\n",
    "        return 0\n",
    "    else:\n",
    "        entity = embedding_def[\"entityType\"]\n",
    "        config = embedding_def[\"config\"]\n",
    "        provider = embedding_def[\"provider\"]\n",
    "        modelName = embedding_def[\"model\"]\n",
    "        dimensions = (\n",
    "            embedding_def[\"dimensions\"] if \"dimensions\" in embedding_def else None\n",
    "        )\n",
    "        index = embedding_def[\"index\"]\n",
    "\n",
    "        if \"huggingface\" == provider:\n",
    "            model = SentenceTransformer(modelName)\n",
    "            llmclient = None\n",
    "        else:\n",
    "            model = None\n",
    "            if \"openai\" == provider:\n",
    "                assert \"OPENAI_API_KEY\" in os.environ, \"OPENAI_API_KEY must be defined\"\n",
    "                llmclient = OpenAI(\n",
    "                    # This is the default and can be omitted\n",
    "                    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    "                )\n",
    "            elif \"mistral\" == provider:\n",
    "                assert (\n",
    "                    \"MISTRAL_API_KEY\" in os.environ\n",
    "                ), \"MISTRAL_API_KEY must be defined\"\n",
    "                llmclient = MistralClient(api_key=os.environ.get(\"MISTRAL_API_KEY\"))\n",
    "        total = 0\n",
    "\n",
    "        template = compiler.compile(config[\"template\"])\n",
    "        # inject uid in the query\n",
    "        # querypart = re.sub(r'([a-zA-Z_]+)',rf\"\\1:{entity}.\\1\",config['query'])\n",
    "        querypart = config[\"dqlQuery\"]\n",
    "        querypart = querypart.replace(\"{\", \"{ uid \", 1)\n",
    "\n",
    "        # remove index by updating DQL schema\n",
    "        clearIndex(predicate)\n",
    "        print(\n",
    "            f\"compute embeddings for {predicate} using  model {modelName} from {provider}\"\n",
    "        )\n",
    "        if only_missing == True:\n",
    "            filter = f\"@filter( NOT has({predicate}))\"\n",
    "        else:\n",
    "            filter = \"\"\n",
    "        # Run query.\n",
    "        after = \"\"\n",
    "        while True:\n",
    "            print(f\"\\r{total} processed\", end=\"\")\n",
    "            txn = client.txn(read_only=True)\n",
    "            query = f\"{{list(func: type({entity}),first:100 {after}) {filter}  {querypart} }}\"\n",
    "            try:\n",
    "                res = txn.query(query)\n",
    "                data = json.loads(res.json)\n",
    "            except Exception as inst:\n",
    "                print(type(inst))  # the exception type\n",
    "                print(inst.args)  # arguments stored in .args\n",
    "                print(inst)\n",
    "                break\n",
    "            finally:\n",
    "                txn.discard()\n",
    "\n",
    "            if len(data[\"list\"]) > 0:\n",
    "                last_uid = data[\"list\"][-1][\"uid\"]\n",
    "                after = f\",after:{last_uid}\"\n",
    "            else:\n",
    "                break\n",
    "\n",
    "            nquads = computeEmbedding(\n",
    "                predicate,\n",
    "                data[\"list\"],\n",
    "                template,\n",
    "                provider,\n",
    "                modelName,\n",
    "                model,\n",
    "                llmclient,\n",
    "                dimensions,\n",
    "            )\n",
    "            mutate_rdf(nquads, client)\n",
    "            total += len(data[\"list\"])\n",
    "        print(f\"\\r{total} processed\")\n",
    "        computeIndex(predicate, index)\n",
    "\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc96c81",
   "metadata": {},
   "source": [
    "With the embedding logic in place we can now define the embeddings we want to compute.\n",
    "\n",
    "The embeddings are computed using the Hugging Face Sentence Transformer model `all-MiniLM-L6-v2`. \n",
    "\n",
    "The `template` is a handlebars template that generates the text to be embedded from the `dqlQuery` result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ddf46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_config = [\n",
    "    {\n",
    "        \"entityType\": \"Product\",\n",
    "        \"attribute\": \"embedding\",\n",
    "        \"index\": \"hnsw\",\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"config\": {\"dqlQuery\": \"{ title: Product.Title}\", \"template\": \"{{title}} \"},\n",
    "    },\n",
    "    {\n",
    "        \"entityType\": \"age_group\",\n",
    "        \"attribute\": \"embedding\",\n",
    "        \"index\": \"hnsw\",\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"config\": {\"dqlQuery\": \"{ value: age_group.Value}\", \"template\": \"{{value}} \"},\n",
    "    },\n",
    "    {\n",
    "        \"entityType\": \"brand\",\n",
    "        \"attribute\": \"embedding\",\n",
    "        \"index\": \"hnsw\",\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"config\": {\"dqlQuery\": \"{ value: brand.Value}\", \"template\": \"{{value}} \"},\n",
    "    },\n",
    "    {\n",
    "        \"entityType\": \"category\",\n",
    "        \"attribute\": \"embedding\",\n",
    "        \"index\": \"hnsw\",\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"config\": {\"dqlQuery\": \"{ value: category.Value}\", \"template\": \"{{value}} \"},\n",
    "    },\n",
    "    {\n",
    "        \"entityType\": \"characteristic\",\n",
    "        \"attribute\": \"embedding\",\n",
    "        \"index\": \"hnsw\",\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"config\": {\n",
    "            \"dqlQuery\": \"{ value: characteristic.Value}\",\n",
    "            \"template\": \"{{value}} \",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"entityType\": \"color\",\n",
    "        \"attribute\": \"embedding\",\n",
    "        \"index\": \"hnsw\",\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"config\": {\"dqlQuery\": \"{ value: color.Value}\", \"template\": \"{{value}} \"},\n",
    "    },\n",
    "    {\n",
    "        \"entityType\": \"material\",\n",
    "        \"attribute\": \"embedding\",\n",
    "        \"index\": \"hnsw\",\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"config\": {\"dqlQuery\": \"{ value: material.Value}\", \"template\": \"{{value}} \"},\n",
    "    },\n",
    "    {\n",
    "        \"entityType\": \"measurement\",\n",
    "        \"attribute\": \"embedding\",\n",
    "        \"index\": \"hnsw\",\n",
    "        \"provider\": \"huggingface\",\n",
    "        \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        \"config\": {\"dqlQuery\": \"{ value: measurement.Value}\", \"template\": \"{{value}} \"},\n",
    "    },\n",
    "]\n",
    "\n",
    "for embedding_def in embedding_config:\n",
    "    buildEmbeddings(embedding_def, only_missing=True)\n",
    "    print(\n",
    "        f\"Embeddings done for {embedding_def['entityType']}.{embedding_def['attribute']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134702e",
   "metadata": {},
   "source": [
    "### Querying the graph using Dgraph similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "7afab3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"result\": [\n",
      "        {\n",
      "            \"category\": \"wedding decor\",\n",
      "            \"products\": [\n",
      "                {\n",
      "                    \"name\": \"Romantic LED Light Valentine's Day Sign\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"category\": \"home decor\",\n",
      "            \"products\": [\n",
      "                {\n",
      "                    \"name\": \"Fall Pillow Covers\"\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"category\": \"home garden balcony decor\",\n",
      "            \"products\": [\n",
      "                {\n",
      "                    \"name\": \"Flower Pot Stand\"\n",
      "                }\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"looking for something to make my home pretty\"\n",
    "\n",
    "# Get the sentence embedding with the same model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "sentence_embedding = model.encode(sentence).tolist()\n",
    "\n",
    "# Use Dgraph similar_to function to find similar categories and use Graph relations to get the products for this category\n",
    "txn = client.txn(read_only=True)\n",
    "query = f\"\"\"\n",
    "  {{  \n",
    "    result(func: similar_to(category.embedding,3,\"{sentence_embedding}\")) {{ \n",
    "        category:category.Value \n",
    "        products:~Product.category (first:3){{ \n",
    "            name:Product.Name\n",
    "        }} \n",
    "    }} \n",
    "  }}\"\"\"\n",
    "try:\n",
    "    res = txn.query(query)\n",
    "    data = json.loads(res.json)\n",
    "    print(json.dumps(data, indent=4))\n",
    "finally:\n",
    "    txn.discard()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c41346",
   "metadata": {},
   "source": [
    "### Extracting entities from the prompt\n",
    "\n",
    "In the previous example, we assumed that the question was about a category. \n",
    "\n",
    "We can go further and use an LLM model to analyze the user prompt and determine the correct criteria to use before querying the graph structure.\n",
    "\n",
    "We will use OpenAI and a prompt build with our knowledge of the graph structure, i.e the description of the entities and predicates that can be found in the graph (aka ontology)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0983fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = [\n",
    "    {\n",
    "        \"entity_name\": \"Product\",\n",
    "        \"description\": \"Item detailed type\",\n",
    "        \"predicates\": {\n",
    "            \"category\": {\n",
    "                \"description\": \"Item category, for example 'home decoration', 'women clothing', 'office supply'\"\n",
    "            },\n",
    "            \"color\": {\"description\": \"color of the item\"},\n",
    "            \"brand\": {\"description\": \"if present, brand of the item\"},\n",
    "            \"characteristic\": {\n",
    "                \"description\": \"if present, item characteristics, for example 'waterproof', 'adhesive', 'easy to use'\"\n",
    "            },\n",
    "            \"measurement\": {\"description\": \"if present, dimensions of the item\"},\n",
    "            \"age_group\": {\n",
    "                \"description\": \"target age group for the product, one of 'babies', 'children', 'teenagers', 'adults'.\"\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "def ontologyPrompt(ontology):\n",
    "    # Create a textual description of the ontology to help prompting LLM\n",
    "    # The graph database has the following entities and predicates:\n",
    "    entities = [f'\\'{e[\"entity_name\"]}\\'' for e in ontology]\n",
    "    list_entities = \", \".join(entities)\n",
    "    s = f\"Identify if the user question is about one of the entities {list_entities}.\"\n",
    "    s += \"\\nIdentify criteria about predicates depending on the entity.\"\n",
    "    for e in ontology:\n",
    "        pred = [f\"'{p}'\" for p in e[\"predicates\"]]\n",
    "        pred_list = \", \".join(pred)\n",
    "        s += f'\\nFor \\'{e[\"entity_name\"]}\\' look for:'\n",
    "        for p in e[\"predicates\"]:\n",
    "            s += f'\\n- \\'{p}\\': {e[\"predicates\"][p][\"description\"]}'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c9fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"\"\"\n",
    "    You are analyzing user prompt to fetch information from a knowledge graph. \n",
    "\n",
    "    {ontologyPrompt(entities)}\n",
    "\n",
    "    Return a json object following the example:\n",
    "    {{\n",
    "        \"entity\": \"product\",\n",
    "        \"intent\": \"one of 'list', 'count'\",\n",
    "        criteria: [\n",
    "        {{ \"predicate\": \"category\", \"value\": \"clothing\"}},\n",
    "        {{ \"predicate\": \"color\", \"value\": \"blue\"}},\n",
    "        {{ \"predicate\": \"age_group\", \"value\": \"adults\"}}   \n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "    If there are no relevant entities in the user prompt, return an empty json object.\n",
    "\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83100e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "llm = OpenAI(\n",
    "    api_key=os.environ.get(\n",
    "        \"OPENAI_API_KEY\", \"<your OpenAI API key if not set as env var>\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Define the entities to look for\n",
    "def text_to_intent(prompt, model=\"gpt-4o-mini\"):\n",
    "    completion = llm.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        response_format={\"type\": \"json_object\"},\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    intent = json.loads(completion.choices[0].message.content)\n",
    "    intent[\"prompt\"] = prompt\n",
    "    return intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "c96bfc42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"entity\": \"product\",\n",
      "        \"intent\": \"list\",\n",
      "        \"criteria\": [\n",
      "            {\n",
      "                \"predicate\": \"category\",\n",
      "                \"value\": \"clothing\"\n",
      "            },\n",
      "            {\n",
      "                \"predicate\": \"color\",\n",
      "                \"value\": \"dark\"\n",
      "            },\n",
      "            {\n",
      "                \"predicate\": \"age_group\",\n",
      "                \"value\": \"teenagers\"\n",
      "            }\n",
      "        ],\n",
      "        \"prompt\": \"do you have clothes for teenagers in dark colors?\"\n",
      "    },\n",
      "    {\n",
      "        \"entity\": \"product\",\n",
      "        \"intent\": \"list\",\n",
      "        \"criteria\": [\n",
      "            {\n",
      "                \"predicate\": \"color\",\n",
      "                \"value\": \"pink\"\n",
      "            },\n",
      "            {\n",
      "                \"predicate\": \"age_group\",\n",
      "                \"value\": \"children\"\n",
      "            }\n",
      "        ],\n",
      "        \"prompt\": \"Which pink items are suitable for children?\"\n",
      "    },\n",
      "    {\n",
      "        \"entity\": \"product\",\n",
      "        \"intent\": \"list\",\n",
      "        \"criteria\": [\n",
      "            {\n",
      "                \"predicate\": \"characteristic\",\n",
      "                \"value\": \"waterproof\"\n",
      "            },\n",
      "            {\n",
      "                \"predicate\": \"category\",\n",
      "                \"value\": \"gardening gear\"\n",
      "            }\n",
      "        ],\n",
      "        \"prompt\": \"Help me find gardening gear that is waterproof\"\n",
      "    },\n",
      "    {\n",
      "        \"entity\": \"product\",\n",
      "        \"intent\": \"list\",\n",
      "        \"criteria\": [\n",
      "            {\n",
      "                \"predicate\": \"measurement\",\n",
      "                \"value\": \"100x50\"\n",
      "            },\n",
      "            {\n",
      "                \"predicate\": \"category\",\n",
      "                \"value\": \"home decoration\"\n",
      "            }\n",
      "        ],\n",
      "        \"prompt\": \"I'm looking for a bench with dimensions 100x50 for my living room\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "example_queries = [\n",
    "    \"do you have clothes for teenagers in dark colors?\",\n",
    "    \"Which pink items are suitable for children?\",\n",
    "    \"Help me find gardening gear that is waterproof\",\n",
    "    \"I'm looking for a bench with dimensions 100x50 for my living room\",\n",
    "]\n",
    "\n",
    "intent_list = [text_to_intent(q) for q in example_queries]\n",
    "\n",
    "print(json.dumps(intent_list, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3c1ad",
   "metadata": {},
   "source": [
    "### Generating queries\n",
    "\n",
    "Now that we know what to look for, we can generate the corresponding Cypher queries to query our database. \n",
    "\n",
    "However, the entities extracted might not be an exact match with the data we have, so we will use the GDS cosine similarity function to return products that have relationships with entities similar to what the user is asking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "248dc911",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "27d4ae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use same embedding model for user input and for the searched entities\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def create_embedding(text):\n",
    "    # print (f\"create embedding for {text}\")\n",
    "    return huggingfaceEmbeddings(model, [text])[0]\n",
    "\n",
    "\n",
    "# algo to create query\n",
    "# for each criteria, compute an embedding of the criteria value.\n",
    "# build a sequence of var block to find the most similar node (category, characteristic, brand etc ...).\n",
    "# build a filter to keep only the Product with the corresponding category, characteristic, brand etc ...\n",
    "\n",
    "\n",
    "def intent_to_dql(intent):\n",
    "    vect = []\n",
    "    vars = []\n",
    "    filters = []\n",
    "    variables = {}\n",
    "    for criteria in intent[\"criteria\"]:\n",
    "        variables[f\"${criteria['predicate']}vect\"] = (\n",
    "            f\"{create_embedding(criteria['value'])}\"\n",
    "        )\n",
    "        vect.append(f\"${criteria['predicate']}vect: float32vector\")\n",
    "        vars.append(\n",
    "            f\"{criteria['predicate']} as var(func:similar_to({criteria['predicate']}.embedding,1,${criteria['predicate']}vect))\"\n",
    "        )\n",
    "        filters.append(\n",
    "            f\"uid_in(Product.{criteria['predicate']}, uid({criteria['predicate']}))\"\n",
    "        )\n",
    "    all_filters = \"\\n AND \".join(filters)\n",
    "    all_vars = \"\\n\".join(vars)\n",
    "    query = f\"\"\"\n",
    "      query test({','.join(vect)}){{\n",
    "          {all_vars}\n",
    "          products(func:type(Product)) @filter( {all_filters} ) {{\n",
    "            name:Product.Name\n",
    "            title:Product.Title\n",
    "            age_group:Product.age_group  {{\n",
    "               value:age_group.Value\n",
    "            }}\n",
    "            brand:Product.brand  {{\n",
    "               value:brand.Value\n",
    "            }}\n",
    "            color:Product.color  {{\n",
    "               value:color.Value\n",
    "            }}\n",
    "            category:Product.category  {{\n",
    "               value:category.Value\n",
    "            }}\n",
    "            characteristic:Product.characteristic  {{\n",
    "               value:characteristic.Value\n",
    "            }}\n",
    "            material:Product.material  {{\n",
    "               value:material.Value\n",
    "            }}\n",
    "            measurement:Product.measurement  {{\n",
    "               value:measurement.Value\n",
    "            }}\n",
    "\n",
    "          }}\n",
    "      }}\n",
    "    \"\"\"\n",
    "    return {\"query\": query, \"variables\": variables}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44798b5",
   "metadata": {},
   "source": [
    "### Generating a response from the retrieved sub-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07260911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(prompt, payload):\n",
    "    model = \"gpt-4o-mini\"\n",
    "    rag_prompt = f\"\"\"\n",
    "        You are suggesting products based on user input and available items.\n",
    "        Reply to the user with suggestions from the following data that match the criteria\n",
    "        {payload}\n",
    "        if possible explain why the items are suggested.\n",
    "        If there are no relevant items reply that we don't have any items that match the criteria.\n",
    "    \"\"\"\n",
    "    completion = llm.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": rag_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a60f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reply(sentence):\n",
    "    intent = text_to_intent(sentence)\n",
    "    dql = intent_to_dql(intent)\n",
    "    res = client.txn(read_only=True).query(dql[\"query\"], variables=dql[\"variables\"])\n",
    "    payload = json.loads(res.json)\n",
    "    return rag(sentence, payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "fc2159df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "> Which pink items are suitable for children?\n",
      "\n",
      "I have two great suggestions for pink items that are suitable for children:\n",
      "\n",
      "1. **Suitcase Music Box**\n",
      "   - **Title**: Suitcase Music Box, Mini Music Box Clockwork Music Box for Children\n",
      "   - **Age Group**: Children\n",
      "   - **Color**: Pink\n",
      "   - **Category**: Toys & Games\n",
      "   - **Characteristics**: This music box features a make-up mirror, jewelry box functionality, requires no batteries, operates with a clockwork mechanism, and has a storage compartment. It's a delightful toy that can also serve as a charming decorative piece for a child's room.\n",
      "\n",
      "2. **Unicorn Curtains**\n",
      "   - **Title**: Eiichuang Unicorn Curtains Rod Pocket Blackout Cute Cartoon Pink Unicorn Wearing a Crown Mermaid Pattern Print Room Darkening Window Drapes for Kids Girls Bedroom Nursery, 2 Panels Set, 29 x 63 Inch\n",
      "   - **Age Group**: Children\n",
      "   - **Color**: Pink\n",
      "   - **Category**: Home Decoration\n",
      "   - **Characteristics**: These curtains feature a fun unicorn design and are room darkening with a rod pocket for easy hanging. They are perfect for a child's bedroom or nursery, creating a whimsical atmosphere.\n",
      "\n",
      "Both items are not only visually appealing with their pink color but also serve functional purposes for children's enjoyment and room decor.\n",
      "\n",
      "> Do you have a helmet with anti allergic padding?\n",
      "\n",
      "Yes, we have a helmet that features anti-allergic interior padding. I recommend the **Steelbird Hi-Gn SBH-11 HUNK Helmet**. \n",
      "\n",
      "Here are some details about it:\n",
      "- **Brand**: Steelbird\n",
      "- **Color**: Glossy Black and Blue\n",
      "- **Measurement**: 580 mm (M)\n",
      "- **Category**: Motorcycle gear\n",
      "- **Characteristics**: \n",
      "  - Anti Allergic Interior\n",
      "  - High Impact ABS Material Shell\n",
      "  - Italian Design Hygienic Interior\n",
      "  - Neck Protector For Extra Comfort\n",
      "  - Multipored Breathable Padding\n",
      "  - Multi-layer EPS (Thermocol)\n",
      "  - Replaceable and washable interior\n",
      "  - Anti-bacteria coating\n",
      "\n",
      "This helmet not only provides anti-allergic features but also has a variety of other comfort and safety attributes, making it a great choice for your motorcycle gear.\n"
     ]
    }
   ],
   "source": [
    "example_queries = [\n",
    "    \"Which pink items are suitable for children?\",\n",
    "    \"Do you have a helmet with anti allergic padding?\",\n",
    "]\n",
    "for q in example_queries:\n",
    "    print()\n",
    "    print(f\"> {q}\")\n",
    "    print()\n",
    "    r = reply(q)\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d30aeb",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You can leverage the knowledge stored in a Graph database to offer accurate responses or recommendations to your user requests.\n",
    "\n",
    "In this example, we have used AI techniques and Graph capabilities to achieve this result.\n",
    "\n",
    "- use a Large Language Model and the graph metadata (ontology) to analyze the user prompt and build an \"intent\".\n",
    "- use a small language model to generate text embeddings for graph entities.\n",
    "- use graph similarity search function to find matching criteria\n",
    "-  use graph traversal and filters to identify matching items\n",
    "- use an LLM to generate a textual response based on the user prompt and the information retrieved from the graph database.\n",
    "\n",
    "This example provides a general working flow of RAG over Graph use case. It can be improved in various points, including\n",
    "- use a different embedding model\n",
    "- create a more complex intent structure covering aggregation, counting and complex criteria.( E.g could you build an intent for the question \"How many products do you have in home decoration under 100$?\" )\n",
    "- train a model to generate the query instead of crafting it.\n",
    "- train a model to generate intent from user input instead of using an LLM\n",
    "- ...\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
